<title>Q-network Pacman 3x3</title>
<h1><center>Pacman 3x3</center></h1>
<h1><center>
    Open DevTools to see Log <a id="Chart-Link" target="_blank">[Training...]</a><br>
    Using q-network for exploit, and for max(q<sub>t+1</sub>)
</center></h1>
<div>
    <center>
        <img src="https://drive.google.com/thumbnail?id=1p9SVTAJ54kptquYFtKN_YNPteX4x1ThO&sz=w1000"
        style="height:50vh;"><br>
        A Q-network demo by Dan D.Q. Dinh
    </center><br>
    <center>
        JS inside HTML: 
        <a target="_blank" href="https://github.com/pacman3x3/pacman3x3.github.io"
            >https://github.com/pacman3x3/pacman3x3.github.io</a>
    </center>
</div>

<!-- TensorFlow -->
<!--script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/4.22.0/tf.min.js"
    integrity="sha512-pLw8dUVrlqzbcwSDkXmF732uRzE8+VFKaTdoBYCqdvtK3q7wza1sQQQEMJP+jYOER4zU1MusZTT2bRzGVRDGZg==" 
    crossorigin="anonymous" referrerpolicy="no-referrer">
</script-->
<script src="tf.min.js"></script>

<!-- This Demo app -->
<script>
// Q-network test
 
// Shorthands
const log  = console.log;
const logw = console.warn;
const loge = console.error;
 
// Globals
const NROW   = 3;
const NCOL   = 3;
const A_MAX  = 4; // 4 actions clockwise from north
const EP_LEN = 7; // Episode length: 7 actions
const NORTH  = 0;
const EAST   = 1;
const SOUTH  = 2;
const WEST   = 3;
const DIRS   = ["North","East\x20","South","West\x20"];
 
function OX_INDENT_getmaxrewardsum(){}
function $_____UTILS_____(){}
 
// Random
function rand() {
    return Math.random();
}
 
// To str
function str(Obj) {
    return JSON.stringify(Obj);
}
 
// Clone var
function clone(Var) {
    return JSON.parse(JSON.stringify(Var));
}
 
// Convert (S,A) to tensor for input
function saq2tensor(S,A, value){
    // Can't set directly in q-network, fit (tuning) it instead
    var X = S.Env.toString().split(",").map(V => parseFloat(V));
    X.push(S.Actor.row);
    X.push(S.Actor.col);
    X.push(A);
    X = new Array(X);
 
    var Inp = tf.tensor(X);
    var Exp = tf.tensor([[value]]);
    return [Inp,Exp];
}

// Google AI preview
function splitIntoSizeChunks(arr, chunkSize) {
    const result = [];

    for (let i = 0; i < arr.length; i += chunkSize) {
        const chunk = arr.slice(i, i + chunkSize);
        result.push(chunk);
    }
    return result;
}


// Make QuickChart JSON to use in url
function make_quickchart_json(Symbol,Series){
    // Gemini, edited
    const create_range = (start, end, step = 1) => {
        const length = Math.floor((end - start) / step) + 1;
        return Array.from({ length }, (_, index) => start + index * step);
    };
    var Data;

    if (Series.length < 100){
        Data = Series;
    }
    else{
        let chunk_size = Math.floor(Series.length / 100);
        let Chunks = splitIntoSizeChunks(Series,chunk_size);

        for (let i=0; i<Chunks.length; i++){
            Chunks[i] = Chunks[i].reduce((sum, cur) => sum + cur, 0);
            Chunks[i] /= chunk_size;
        }

        Data = Chunks;
    }

    // QuickChart line chart
    var Obj = {
        "type": "line",
        "data": {
            "labels": create_range(0,Data.length),
            "datasets": [{
                "label": Symbol,
                "data": [0].concat(Data)
            }]
        }
    };
    var Json = JSON.stringify(Obj);

    // Remove doublequotes for QuickChart to work
    Json = Json.replaceAll(`"type"`,"type");
    Json = Json.replaceAll(`"data"`,"data");
    Json = Json.replaceAll(`"labels"`,"labels");
    Json = Json.replaceAll(`"datasets"`,"datasets");
    Json = Json.replaceAll(`"label"`,"label");
    Json = Json.replaceAll(`"`, `'`);
    return Json;
}
 
function $_____CLASS_____(){}
 
// Q-network
// Bellman: Q = Q + B*M = Q + B*(R+D*max(Qnext) - Q)
// B is learning rate, M is temporal difference.
// Coding convention exception: Bellman vars are all in uppercase
class qnetwork {
    static Env; 
    static Actor; 
    static Orig_Env;
    static Orig_Actor;
    static Network;    
    static B;             // Learning rate
    static D;             // Discount factor
    static Rmax;          // Max possible sum of rewards
    static episodes;      // Number of episodes (number of runs through the episode)    
    static xrate;         // Current explore rate
    static xrate_change;  // Reduction of xrate after each action
    static tuning_epochs; // For learning updated q-value
    static qnet_lr;       // Q-network learning rate
    static Rsumlog = [];
 
    // Set a value in q-network
    // S: State of both env and actor
    // A: Action (0..3) clockwise from north
    static async set_qvalue(States,Actions, Qvalues){
        var Inp=[], Exp=[];
 
        for (let i=0; i<States.length; i++){
            let [Inp_Temp,Exp_Temp] = saq2tensor(States[i],Actions[i], Qvalues[i]);
            Inp = Inp.concat(Inp_Temp.arraySync());
            Exp = Exp.concat(Exp_Temp.arraySync());
        }       
        Inp = tf.tensor(Inp);
        Exp = tf.tensor(Exp);    
 
        var Fit_Res = await thisclass.Network.fit(Inp,Exp,{ 
            epochs:thisclass.tuning_epochs, batchSize:States.length, callbacks:{}
        });
    }
 
    // Get a value in q-network
    // S: State of both env and actor
    // A: Action (0..3) clockwise from north
    static async get_qvalue(S,A){
        var [Inp,Exp] = saq2tensor(S,A,0);
        var U         = (await thisclass.Network.predict(Inp)).arraySync();
        return U[0][0];
    }
 
    // Q-function 
    // Returns estimated sum of Reward by action A and future Rewards
    static async q(S,A){
        return await thisclass.get_qvalue(S,A);
    }
 
    // Get current state or custom state
    // Returns a string presents the state (of both env and actor)
    static s(Env=null, Actor=null){
        if (Env==null || Actor==null)
            return {Env:clone(thisclass.Env), Actor:clone(thisclass.Actor)};
 
        return {Env:clone(Env),Actor:clone(Actor)};        
    }    
 
    // Reward of taking action A at state S
    static r(S,A, log_validity=true){
        var Env   = S.Env;
        var Actor = S.Actor;
 
        // Check if Actor is at invalid coords (where Env value is 1, actor can't be there)
        if (Env[Actor.row][Actor.col] != 0){
            if (log_validity)
                loge(`qnetwork.r: Bad actor coords, Env=${str(Env)}, Actor=${str(Actor)}`);
 
            return -1;
        }
 
        // Check if action 'a' violates the rules
        // Can't go North
        if (Actor.row==0 && A==NORTH){
            if (log_validity)
                loge(`qnetwork.r: Can't go north, Env=${str(Env)}, Actor=${str(Actor)}, A=${A}`);
 
            return -1;
        }
        // Can't go East
        if (Actor.col==NCOL-1 && A==EAST){
            if (log_validity)
                loge(`qnetwork.r: Can't go east,\x20 Env=${str(Env)}, Actor=${str(Actor)}, A=${A}`);
 
            return -1;
        }
        // Can't go South
        if (Actor.row==NROW-1 && A==SOUTH){
            if (log_validity)
                loge(`qnetwork.r: Can't go south, Env=${str(Env)}, Actor=${str(Actor)}, A=${A}`);
 
            return -1;
        }
        // Can't go West
        if (Actor.col==0 && A==WEST){
            if (log_validity)
                loge(`qnetwork.r: Can't go west,\x20 Env=${str(Env)}, Actor=${str(Actor)}, A=${A}`);
 
            return -1;
        }
 
        // All valid, now get reward
        if (A==NORTH)
            return Env[Actor.row-1][Actor.col];
        if (A==EAST)
            return Env[Actor.row  ][Actor.col+1];
        if (A==SOUTH)
            return Env[Actor.row+1][Actor.col];
        if (A==WEST)
            return Env[Actor.row  ][Actor.col-1];
    }
 
    // Apply action
    static apply_action(Env,Actor,A){
        var R = r(s(Env,Actor), A);
 
        // Invalid action
        if (R==-1){
            loge(`qnetwork.apply_action: Invalid action, Env=${str(Env)}, Actor=${str(Actor)}, A=${A}`);
            return new Error("bad-action");
        }
 
        // Change actor coords
        if (A==NORTH)
            Actor.row--;
        else
        if (A==EAST)
            Actor.col++;
        else
        if (A==SOUTH)
            Actor.row++;
        else
        if (A==WEST)
            Actor.col--;
 
        // Pacman eats the food at his coords
        var row       = Actor.row;
        var col       = Actor.col;
        Env[row][col] = 0;
        return clone(Actor);
    }
 
    // Get valid actions
    static get_valid_actions(S){
        var Possible_Actions = [];
        var log_validity;
 
        for (let A=0; A<A_MAX; A++)
            if (r(S,A, log_validity=false) >= 0)
                Possible_Actions.push(A);
 
        return Possible_Actions;
    }
 
    // Get random valid action
    static get_rand_action(S){
        var Actions = thisclass.get_valid_actions(S);
        var n       = Actions.length;
        var i       = Math.floor(rand()*n);
        return Actions[i];
    }
 
    // Get best action based on q-network
    static async get_best_action(S){    
        var Valid_Actions = thisclass.get_valid_actions(S);
        var Qmax          = -Number.MAX_SAFE_INTEGER;
        var Amax;
 
        for (let A=0; A<A_MAX; A++){
            let Q = await q(S,A); // Correct, using q-value
 
            if (Valid_Actions.indexOf(A)>=0 && Q>Qmax){
                Qmax = Q;
                Amax = A;
            }
        }
        return Amax;
    }
 
    // Q-network update at S:A (whole episode)
    static async q_update(Env_States,Actor_States, Actions,Target_Qs){
        // Train q-network by whole episode experience
        var States = [];

        for (let i=0; i<Env_States.length; i++)
            States.push(s(Env_States[i],Actor_States[i]));

        await thisclass.set_qvalue(States,Actions, Target_Qs);
    }
 
    // Init q-network (sizes: 2^9*9 rows, 4 cols)
    // ie. 512*9*4 values = 18 432 values
    static async init_qnetwork(){
        var Env   = [[0,0,0],[0,0,0],[0,0,0]];
        var Actor = {row:0, col:0};
 
        // Random params (weights, biases) for network
        // Layers 200*100 roughly equals 2^9(env) * 9(locs) * 4(actions)
        // WARNING:
        // DO NOT INIT TO ZEROS, USE RANDOM, THESE ARE PARAMS AND
        // NOT THE DIRECT Q-VALUE OUTPUT AS IN THE Q-TABLE IMPLEMENTATION
        // OR THE NETWORK NEVER CONVERGE AS OUTPUT IS STUCK AT ZERO:
        //   - FEED TO ZERO OUTPUT, LOSS IS NON-ZERO
        //   - BUT DERIVATIVE OF RELU IS ZERO, THUS BACKPROP GRAD * f'(RELU) -> 0 GRADS.
        var X  = tf.input({shape: [9+2+1]});
        var H1 = tf.layers.dense({
                     units:200, activation:'relu', kernelInitializer:'randomUniform',
                     biasInitializer:"randomUniform"
                 }).apply(X);
        var H2 = tf.layers.dense({
                     units:100, activation:'relu', kernelInitializer:'randomUniform',
                     biasInitializer:"randomUniform"
                 }).apply(H1);
        var U  = tf.layers.dense({
                     // Y is qvalue, not action (only policy network has action as Y) 
                     units:1, activation:'linear', kernelInitializer:'randomUniform',
                     biasInitializer:"randomUniform"
                 }).apply(H2);
 
        // Model (regression)
        console.warn("Q-network learning rate:",thisclass.qnet_lr);
        var Model     = tf.model({inputs:X, outputs:U});
        var Optimiser = tf.train.adam(thisclass.qnet_lr);
        Model.compile({
            loss:'meanSquaredError', optimizer:Optimiser
        });
        this.Network = Model;
    }
 
    // Save as original state
    static save_as_orig_state(){
        thisclass.Orig_Env   = clone(thisclass.Env);
        thisclass.Orig_Actor = clone(thisclass.Actor);
    }
 
    // Load original state
    static load_orig_state(){
        thisclass.Env   = clone(thisclass.Orig_Env);
        thisclass.Actor = clone(thisclass.Orig_Actor);
    }
 
    // Log square-one (first Env/Actor state) in q-network
    static async log_training_stats(runs_done, Rsum, avg_rsum){
        // Find best action for original Env/Actor state
        var Env   = clone(thisclass.Orig_Env);
        var Actor = clone(thisclass.Orig_Actor);
        var S     = s(Env,Actor);
        var Abest = await thisclass.get_best_action(S);
 
        // Get q and print
        var Qzero = await q(S,Abest);
        log(`Best Q0: ${Qzero.toFixed(9)}, Rsum: ${Rsum.toFixed(1)}, `+
            `runs done: ${runs_done} / ${thisclass.episodes}, avg_rsum = ${avg_rsum.toFixed(12)}`);
    }
 
    // Train q-network
    static async train_qnetwork(){
        var eps          = thisclass.episodes;
        var xrate        = thisclass.xrate;
        var xrate_change = thisclass.xrate_change;
        var Rsum, Actions;
        thisclass.save_as_orig_state();
        var rsum_all_runs = 0;
 
        // Run 'episodes' times
        for (let run=0; run<eps; run++){
            // UI
            let Ele = document.querySelector("#Chart-Link");
            Ele.innerHTML = `[Run ${run+1}<sub><small>explore`+
                `${Math.floor(Math.max(xrate*100,10))}</small></sub> / ${eps}]`;

            // Traing
            thisclass.load_orig_state();
            Rsum    = 0;
            Actions = [];
            let Env_States = [];
            let Actor_States = [];
            let Target_Qs = [];
 
            // Do EP_LEN steps
            for (let step=0; step<EP_LEN; step++){
 
                // Choose Explore or Exploit
                // Leave 10% explore always at the end, avoid failure at
                // training end.
                // to_explore = rand() < xrate + .1, is not good coz
                // 10% of runs at beginning is always exploring.
                let to_explore = rand() < Math.max(xrate, .1);
 
                if (to_explore){
                    let A = thisclass.get_rand_action(s());
                    // Can't q_update per action as in q-table, 
                    // to much calculation in q-network, q_update after whole episode instead.
                    let R = r(s(),A);
                    Env_States.push(thisclass.Env);
                    Actor_States.push(thisclass.Actor);
                    Actions.push(A);

                    // qTarget = q1 + rr*td
                    // td = [rw1 + dc*q2max] - q1
                    let rw1 = R;
                    let q1 = await q(s(),A);

                    thisclass.apply_action(thisclass.Env,thisclass.Actor,A);
                    let a2max = await thisclass.get_best_action(s());
                    let q2max = await q(s(),a2max);
                    let td = (rw1 + thisclass.D*q2max) - q1; // Unused
                    let q_update = q1 + thisclass.B * td; // Unused

                    // Q-network has optimizer, it knows q1 indirectly as the
                    // result of the network, just supply q_target to it is enough,
                    // and it has its own lr to replace the q-table's learning rate.
                    // Applying full q2 to q-network will make it confused due to
                    // being applied with Bellman logic (q-table algorithmic logic)
                    // and gradient-based optimization.
                    let q_target = rw1 + thisclass.D*q2max
                    Target_Qs.push(q_target); // Use q_target instead of q2

                    Rsum += R;
                }
                else{
                    let A = await thisclass.get_best_action(s()); // Based on q-network
                    // Can't q_update per action as in q-table, 
                    // to much calculation in q-network, q_update after whole episode instead.
                    let R = r(s(),A);
                    Env_States.push(thisclass.Env);
                    Actor_States.push(thisclass.Actor);
                    Actions.push(A);

                    // qTarget = q1 + rr*td
                    // td = [rw1 + dc*q2max] - q1
                    let rw1 = R;
                    let q1 = await q(s(),A);

                    thisclass.apply_action(thisclass.Env,thisclass.Actor,A);
                    let a2max = await thisclass.get_best_action(s());
                    let q2max = await q(s(),a2max);
                    let td = (rw1 + thisclass.D*q2max) - q1; // Unused
                    let q_update = q1 + thisclass.B * td; // Unused

                    // Q-network has optimizer, it knows q1 indirectly as the
                    // result of the network, just supply q_target to it is enough,
                    // and it has its own lr to replace the q-table's learning rate.
                    // Applying full q2 to q-network will make it confused due to
                    // being applied with Bellman logic (q-table algorithmic logic)
                    // and gradient-based optimization.
                    let q_target = rw1 + thisclass.D*q2max
                    Target_Qs.push(q_target); // Use q_target instead of q2

                    Rsum += R;

                }
                xrate -= xrate_change;
            } // step
            thisclass.Rsumlog.push(Rsum)
 
            // q_update after whole run thru episode
            thisclass.load_orig_state();
            await thisclass.q_update(Env_States,Actor_States,Actions,Target_Qs);
 
            // Log training result
            rsum_all_runs += Rsum;
            let avg_rsum = rsum_all_runs / (run+1);

            if (run==0 || (run+1)%thisclass.log_after==0)
                await thisclass.log_training_stats(run+1, Rsum, avg_rsum);
        } // run

        // Chart of episode reward sum
        var Quickchart_Cfg = make_quickchart_json("Rsum",thisclass.Rsumlog);
        var Url = `https://quickchart.io/chart?c=${Quickchart_Cfg}`
        var Ele = document.querySelector("#Chart-Link")
        Ele.setAttribute("href",Url);
        Ele.innerHTML = "\x20& see Chart";
    }
 
    // Infer from initial env state and actor state
    static async infer(){
        thisclass.load_orig_state();
        log(`Original state: ${str(s())}`);
        var Rtotal     = 0;
        var Rmax       = thisclass.Rmax;
        var Coords_Log = [];
        var Orig_Env   = clone(thisclass.Orig_Env);
 
        for (let i=0; i<EP_LEN; i++){
            let S     = s();
            let Abest = await thisclass.get_best_action(S); // From q-network
            Rtotal   += r(S,Abest);
            let Actor = thisclass.apply_action(thisclass.Env, thisclass.Actor, Abest);
            Coords_Log.push(Actor);
            log(`Go to: ${DIRS[Abest]}, total R: ${Rtotal}, max R: ${Rmax}, `+
                `state after: ${str(s())}`);            
        }
 
        // Make visual grid to see
        var Grid = [["..","..",".."],["..","..",".."],["..","..",".."]];
 
        for (let i=0; i<Coords_Log.length; i++){
            let row = Coords_Log[i].row;
            let col = Coords_Log[i].col;
 
            if (Orig_Env[row][col]==0)
                Grid[row][col] = "00";
            else
                Grid[row][col] = "YY";
        }
        Grid[0][0] = ">>";
 
        log("\nActor travel path (>> is start, 00 is no R, YY is with R):");
        log(Grid[0].join("\x20"));
        log(Grid[1].join("\x20"));
        log(Grid[2].join("\x20"));
    }
 
    // Run
    static async run(){
        // Data: Minimal Pacman 3x3 (2x2 is rather no meaning of circle)
        //       Sample episode: Run in circle for max foods 
        thisclass.Env   = [[0, 1, 1],
                           [1, 0, 1],
                           [1, 1, 1]];
        thisclass.Actor = {row:0, col:0};
        var Orig_Env    = clone(thisclass.Env);
        var Orig_Actor  = clone(thisclass.Actor);
 
        function get_max_reward_sum(){
            var sum = 0; 
 
            for (let i=0; i<NROW; i++)
                for (let j=0; j<NCOL; j++)
                    sum += Orig_Env[i][j];
 
            return sum;
        }
 
        // Init Q-network, params to suit: 
        //     Rows: 2^9 (env states) * 9 (actor states)
        //     Cols: 4 (directions to move to)
        thisclass.qnet_lr = 1e-3; // Learning rate q-network, 1e-3 is default in TF.js Adam
        await thisclass.init_qnetwork();    
        thisclass.B = 1e-3;            // Learning rate (Bellman)
        thisclass.D = .8;              // Discount factor
        thisclass.episodes      = 5000; // Run through the episode this number of times
        thisclass.xrate         = 1;   // Starting explore rate, 100%
        thisclass.xrate_change  = 1 / (thisclass.episodes * EP_LEN); // After every action
        thisclass.log_after     = thisclass.episodes / 100;          // Log test result after
        thisclass.tuning_epochs = 1; // Once only, avoid sticking with early q values
        thisclass.Rmax          = get_max_reward_sum();
         
        // Train
        log("Training...");
        await thisclass.train_qnetwork();
 
        // Infer
        log("\nInferring...");
        await thisclass.infer();
 
        log("Done.");
    }
}
const s = qnetwork.s; // For convenient call
const r = qnetwork.r; 
const q = qnetwork.q;
 
const thisclass = qnetwork;
window.onload = function(){
    thisclass.run();
};
</script>
<!-- EOF -->
